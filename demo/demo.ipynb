{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright (c) Microsoft Corporation. All rights reserved.\n",
    "# Licensed under the MIT License."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a Semantic Segmentation Model for Mapping Settlements\n",
    "\n",
    "In this notebook, we will train a semantic segmentation model to identify **buildings** and **solar panels** in aerial imagery.\n",
    "\n",
    "Accurate and up-to-date mapping is essential for supporting humanitarian efforts, particularly in refugee camps where infrastructure monitoring and development planning are critical. Traditional mapping services often fall short in these rapidly changing environments. Semantic segmentation models can accelerate the mapping process, providing detailed information that helps ensure resources are effectively allocated to meet the needs of the population.\n",
    "\n",
    "We will cover the following steps:\n",
    "1. [Creating Semantic Segmentation Masks](#Creating-Semantic-Segmentation-Masks)\n",
    "    - [Collecting Annotations](#collecting-annotations)\n",
    "    - [Burning Masks](#burning-masks)\n",
    "    - [Clipping Masks to Imagery](#clipping-masks-to-imagery)\n",
    "2. [Sampling Chips to Create a Dataset](#sampling-chips-for-semantic-segmentation-dataset)\n",
    "2. [Model Training](#Model-Training)\n",
    "3. [Evaluation and Inference](#evaluation-and-inference)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import glob\n",
    "import importlib.util\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "import geopandas as gpd\n",
    "import matplotlib.colors as colors\n",
    "import matplotlib.patches as mpatches\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import shapely.geometry\n",
    "from shapely.ops import unary_union\n",
    "\n",
    "from src.geo_utils import concat_geo_files, exclude_points_within_buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remount /dev/shm with more shared memory to prevent PyTorch DataLoader worker PID errors.\n",
    "# PyTorch uses shared memory for multiprocessing; increasing its size helps avoid broken pipe issues.\n",
    "command = (\n",
    "    'sudo mount -o size=300000000000 -o nr_inodes=1000000 -o noatime, nodiratime -o remount /dev/shm'\n",
    ")\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## About the Data\n",
    "The data used in this demo is sourced from OpenAerialMap and OpenStreetMap (OSM). We will use an aerial imagery scene downloaded from [OpenAerialMap](https://map.openaerialmap.org/#/34.745614528656006,3.7642264421477485,15/square/12233202121301210/6447e8162155f0000546362b?_k=0r8ow6), which covers a portion of the Kalobeyei Integrated Settlement Camp 2. This scene is provided as a GeoTIFF file and is divided into two spatially disjoint areas: one for training and validation, and the other for testing (Figure 1).\n",
    "\n",
    "We use the bounds of the aerial imagery scene to extract features from OpenStreetMap using the `osmnx` package. Specifically, we retrieve features with the following OSM tags:\n",
    "- `building=True`\n",
    "- `generator:source=solar`\n",
    "- `amenity=toilets`\n",
    "\n",
    "Note that toilets in this area may be tagged either with `building=toilets` or `amenity=toilets`, so we retrieve features with both tags.\n",
    "\n",
    "The raw OSM features do not perfectly align with our imagery scene (Figure 2). To correct this, we use QGIS to manually adjust the polygon features to align with the buildings visible in the imagery. Additionally, the `amenity=toilets` features are returned as point geometries (shown as green dots in Figure 2). We use QGIS to buffer these points into square polygons and align them with the visible toilet structures before merging them with the other polygon layer (Figure 3). We also manually label numerous toilets and solar panels that were not initially labeled.\n",
    "\n",
    "We also draw polygons to represent background areas without buildings, solar panels, or toilets, and label some background points over objects like cars, fences, hedges, and wells to serve as hard negative samples.\n",
    "\n",
    "<div style=\"display: flex; justify-content: space-between;\">\n",
    "\n",
    "<div style=\"text-align: center; width: 32%;\">\n",
    "<img src=\"./assets/traintest_split.jpg\" alt=\"Kalobeyei Camp 2\" style=\"height: 250px; object-fit: cover;\"/>\n",
    "<p>Figure 1: Kalobeyei Camp 2</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; width: 32%;\">\n",
    "<img src=\"./assets/raw_osm_features.png\" alt=\"Raw OSM Features\" style=\"height: 250px; object-fit: cover;\"/>\n",
    "<p>Figure 2: Raw OSM Features</p>\n",
    "</div>\n",
    "\n",
    "<div style=\"text-align: center; width: 32%;\">\n",
    "<img src=\"./assets/cleaned_osm_features.png\" alt=\"Cleaned OSM Features\" style=\"height: 250px; object-fit: cover;\"/>\n",
    "<p>Figure 3: Cleaned OSM Features</p>\n",
    "</div>\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directories for raw, interim, and processed data\n",
    "RAW_DATA_DIR = './data/raw'\n",
    "INTERIM_DATA_DIR = './data/interim'\n",
    "PROCESSED_DATA_DIR = './data/processed'\n",
    "\n",
    "# Paths to raw imagery files\n",
    "TRAINVAL_IMAGE_FILE = f'{RAW_DATA_DIR}/images/trainval_Kalobeyei_2B_Flight_03.tif'\n",
    "TEST_IMAGE_FILE = f'{RAW_DATA_DIR}/images/test_Kalobeyei_2B_Flight_03.tif'\n",
    "\n",
    "# Paths to raw annotation files\n",
    "ANNOTATIONS_FILE = f'{RAW_DATA_DIR}/annotations/osm_buildings-toilets-solar_aligned.geojson'\n",
    "BACKGROUND_POLYGONS_FILE = f'{RAW_DATA_DIR}/annotations/background_polygons.geojson'\n",
    "CAR_POINTS_FILE = f'{RAW_DATA_DIR}/annotations/car_points.geojson'\n",
    "HEDGE_POINTS_FILE = f'{RAW_DATA_DIR}/annotations/hedge_points.geojson'\n",
    "WELL_POINTS_FILE = f'{RAW_DATA_DIR}/annotations/well_points.geojson'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Semantic Segmentation Masks\n",
    "\n",
    "In this section, we will continue preparing our data for training the semantic segmentation model. Our OSM and manually drawn annotations contain vector data for buildings, solar panels, and background regions. However, to train our model, we need pixelwise segmentation masks.\n",
    "\n",
    "### Collecting Annotations\n",
    "The OSM annotation file contains multiple columns with OSM tags for buildings and solar panels, while the background polygons file has only a `class` column with the value \"background\" for all rows. As a first step, we will resolve these inconsistencies to create a uniform polygon dataset.\n",
    "\n",
    "To do this, we will use the `collect_annotations.py` script from this repository. This script performs the following tasks:\n",
    "- Collects building and solar panel polygons from the OSM annotations and background polygons from our manual annotation file.\n",
    "- Creates building boundary polygons by applying a buffer to the exterior of each building polygon to improve separation between closely packed buildings.\n",
    "- Adds a `class` column for all the features.\n",
    "- Combines the features into one dataframe with only the `class` and `geometry` columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define output file targets for collected annotations\n",
    "polygon_annotations_file = f'{INTERIM_DATA_DIR}/annotations/{os.path.basename(ANNOTATIONS_FILE).replace(\".geojson\", \"_polygons.geojson\")}'\n",
    "output_file = f'{INTERIM_DATA_DIR}/semantic_segmentation_classes.gpkg'\n",
    "\n",
    "# Construct the command to collect the annotations\n",
    "command = (\n",
    "    f\"python ../scripts/semantic_segmentation/collect_annotations.py \"\n",
    "    f\"--object-annotations {ANNOTATIONS_FILE} \"\n",
    "    f\"--polygon-annotations {polygon_annotations_file} \"\n",
    "    f\"--background-annotations {BACKGROUND_POLYGONS_FILE} \"\n",
    "    f\"--save-path {output_file}\"\n",
    ")\n",
    "# Display and run the command\n",
    "print(command)\n",
    "subprocess.run(command, shell=True)\n",
    "\n",
    "# Load the collected annotations\n",
    "semantic_segmentation_classes = gpd.read_file(output_file)\n",
    "display(semantic_segmentation_classes.head())\n",
    "display(semantic_segmentation_classes['class'].value_counts())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Burning Masks\n",
    "Now, we will create the semantic segmentation masks by using the `create_mask.py` script. This script burns polygon features into a raster in a specified order, converting our vector data into pixelwise segmentation masks suitable for training our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rasterize the annotations to create masks\n",
    "for image_file in [TRAINVAL_IMAGE_FILE, TEST_IMAGE_FILE]:\n",
    "    # Define the output file path for the mask\n",
    "    output_file = os.path.join(\n",
    "        INTERIM_DATA_DIR,\n",
    "        'unclipped_masks',\n",
    "        os.path.basename(image_file)\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "    # Define the label file and column\n",
    "    label_file = f'{INTERIM_DATA_DIR}/semantic_segmentation_classes.gpkg'\n",
    "    label_column = 'class'\n",
    "\n",
    "    # Define the order of labels for the mask\n",
    "    label_order = [\"background\", \"building\", \"building_boundary\", \"solar\"]\n",
    "    label_order_cli = ' '.join(label_order)  # Convert list to space-separated string for CLI\n",
    "\n",
    "    # Construct the command to create the mask\n",
    "    command = (\n",
    "        f\"python ../scripts/semantic_segmentation/create_mask.py \"\n",
    "        f\"-i {image_file} \"\n",
    "        f\"-l {label_file} \"\n",
    "        f\"--label-column {label_column} \"\n",
    "        f\"-o {output_file} \"\n",
    "        f\"--label-order {label_order_cli}\"\n",
    "    )\n",
    "\n",
    "    # Display and run the command, suppressing its output\n",
    "    print(command)\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us plot the ground truth segmentation mask raster we just created. Each pixel in the raster will be colored according to the class that was burned into the raster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a listed colormap for the mask\n",
    "class_colors = {\n",
    "    \"unlabeled\": \"#D3D3D3\",       # Light Grey\n",
    "    \"background\": \"#D2B48C\",      # Tan\n",
    "    \"building\": \"#0000FF\",        # Blue\n",
    "    \"building_boundary\": \"#FF0000\",  # Red\n",
    "    \"solar\": \"#FFFF00\"            # Yellow\n",
    "}\n",
    "segmentation_cmap = colors.ListedColormap([class_colors[key] for key in class_colors.keys()])\n",
    "\n",
    "\n",
    "# Plot the mask for the training/validation image\n",
    "mask_file = f'{INTERIM_DATA_DIR}/unclipped_masks/{os.path.basename(TRAINVAL_IMAGE_FILE)}'\n",
    "with rasterio.open(mask_file) as src:\n",
    "    mask = src.read(1)\n",
    "\n",
    "plt.figure(figsize=(4, 6))\n",
    "plt.imshow(mask, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "plt.axis('off')\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=segmentation_cmap(i), label=label.replace(\"_\", \" \")) for i, label in enumerate(class_colors.keys())]\n",
    "plt.legend(handles=legend_patches, loc='center left', bbox_to_anchor=(1, 0.5), frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clipping Masks to Imagery\n",
    "\n",
    "The current mask spans the entire bounding box of the train/validation imagery, including areas outside the actual imagery footprint. To accurately confine the mask to the imagery, we will create footprint shapefiles for the train/validation and test imagery files and use these shapefiles to clip the masks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the directory to save the clipped masks\n",
    "footprints_dir = f'{INTERIM_DATA_DIR}/image_footprints/'\n",
    "os.makedirs(footprints_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# List of image files to process\n",
    "for image_file in [TRAINVAL_IMAGE_FILE, TEST_IMAGE_FILE]:\n",
    "    # Define the output file path for the footprint shapefile\n",
    "    output_file = os.path.join(footprints_dir, os.path.basename(image_file).replace('.tif', '_footprint.shp'))\n",
    "    \n",
    "    # Construct the command to generate the footprint shapefile\n",
    "    command = f\"python ../scripts/data_preprocessing/get_raster_footprint.py -f {image_file} -o {output_file}\"\n",
    "    print(command)  # Display the command being run\n",
    "    \n",
    "    # Execute the command, suppressing its output\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "# Plot the footprints\n",
    "print(\"Plotting the image footprints...\")\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "for image_file, color in zip([TRAINVAL_IMAGE_FILE, TEST_IMAGE_FILE], ['green', 'blue']):\n",
    "    footprint_file = os.path.join(footprints_dir, os.path.basename(image_file).replace('.tif', '_footprint.shp'))\n",
    "    extent = gpd.read_file(footprint_file)\n",
    "    extent.plot(ax=ax, facecolor=color, alpha=0.5, edgecolor='none', linewidth=2)\n",
    "\n",
    "# Create custom legend patches\n",
    "train_patch = mpatches.Patch(facecolor='green', alpha=0.5, edgecolor='none', label='Train / Val')\n",
    "test_patch = mpatches.Patch(facecolor='blue', alpha=0.5, edgecolor='none', label='Test')\n",
    "plt.legend(handles=[train_patch, test_patch], loc='upper left', frameon=False)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clip masks to the footprints using gdalwarp cutline\n",
    "print(\"Clipping the masks to the image footprints...\")\n",
    "for image_file in [TRAINVAL_IMAGE_FILE, TEST_IMAGE_FILE]:\n",
    "    # Define the paths to the mask, footprint, and output files\n",
    "    mask_file = os.path.join(\n",
    "        INTERIM_DATA_DIR,\n",
    "        'unclipped_masks',\n",
    "        os.path.basename(image_file)\n",
    "    )\n",
    "    footprint_file = os.path.join(footprints_dir, os.path.basename(image_file).replace('.tif', '_footprint.shp'))\n",
    "    output_file = os.path.join(\n",
    "        RAW_DATA_DIR,\n",
    "        'semantic_segmentation',\n",
    "        'masks',\n",
    "        os.path.basename(image_file)\n",
    "    )\n",
    "    os.makedirs(os.path.dirname(output_file), exist_ok=True) # Ensure the directory exists\n",
    "\n",
    "    # Construct the command to clip the mask\n",
    "    command = f\"gdalwarp -cutline {footprint_file} -crop_to_cutline {mask_file} {output_file}\"\n",
    "\n",
    "    # Display and run the command, suppressing its output\n",
    "    print(command)\n",
    "    subprocess.run(command, shell=True, stdout=subprocess.DEVNULL, stderr=subprocess.DEVNULL)\n",
    "\n",
    "\n",
    "# Plot the mask for the training/validation image\n",
    "print(\"Plotting the clipped mask for the training/validation image...\")\n",
    "mask_file = f'{RAW_DATA_DIR}/semantic_segmentation/masks/{os.path.basename(TRAINVAL_IMAGE_FILE)}'\n",
    "with rasterio.open(mask_file) as src:\n",
    "    mask = src.read(1)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(mask, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "plt.axis('off')\n",
    "\n",
    "legend_patches = [mpatches.Patch(color=segmentation_cmap(i), label=label.replace(\"_\", \" \")) for i, label in enumerate(class_colors.keys())]\n",
    "plt.legend(handles=legend_patches, loc='upper right', bbox_to_anchor=(1, 1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sampling Chips for Semantic Segmentation Dataset\n",
    "\n",
    "In this step, we will extract smaller image chips from our paired imagery and mask scenes to create a dataset for training our model. Pre-sampling these chips helps balance the dataset, ensuring that small features, such as solar panels, which occupy a relatively small area in the scenes, are adequately represented.\n",
    "\n",
    "The `sample_chips` script is responsible for generating these smaller image chips. To tailor the chip sampling process to our specific data, we need to customize the `_get_candidate_points` function. This function implements a sampling strategy to generate diverse chips by:\n",
    "- Including all solar panel centroids, since solar panels are a minority class.\n",
    "- Including building centroids that don't contain solar panels (since those that do will already be present in chips sampled from the solar panel centroids).\n",
    "- Including manually specified as well as randomly drawn background points, ensuring they are not too close to other points.\n",
    "\n",
    "With these candidate chip centroids, the `sample_chips` function then samples the chips.\n",
    "\n",
    "Hereâ€™s the code to override the `_get_candidate_points` function and set up the arguments for the `sample_chips` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to dynamically import a module\n",
    "def import_from_path(module_name, file_path):\n",
    "    \"\"\"\n",
    "    Dynamically import a module from a given file path.\n",
    "    \n",
    "    Args:\n",
    "        module_name (str): Name to assign to the module.\n",
    "        file_path (str or Path): Path to the module file.\n",
    "    \n",
    "    Returns:\n",
    "        module: The imported module.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    spec = importlib.util.spec_from_file_location(module_name, file_path)\n",
    "    module = importlib.util.module_from_spec(spec)\n",
    "    spec.loader.exec_module(module)\n",
    "    return module\n",
    "\n",
    "def _get_candidate_points(seed):\n",
    "    \"\"\"\n",
    "    Helper function to get candidate points for chip sampling.\n",
    "    It loads all polygon annotations and filters for solar and building polygons. Then,\n",
    "    it removes building polygons that contain solar polygons (since these will already\n",
    "    be included in the solar polygons). Next, it subsamples a diverse set of building\n",
    "    polygons (since there are many more building polygons than solar polygons). Then,\n",
    "    it gets centroids of these solar and building polygons.\n",
    "\n",
    "    To get background points, it samples points from footprints and also loads\n",
    "    manually-selected background points. It combines the footprint points and manual\n",
    "    points, making sure to exclude points that are too close to solar or building\n",
    "    centroids.\n",
    "\n",
    "    Returns:\n",
    "    tuple: A tuple containing the solar points, building points, and background points.\n",
    "    \"\"\"\n",
    "    print(\"Getting candidate points for chip sampling...\")\n",
    "    POLYGON_ANNOTATIONS_PATH = (polygon_annotations_file)\n",
    "    FOOTPRINT_PATHS = [\n",
    "        \"./data/interim/image_footprints/trainval_Kalobeyei_2B_Flight_03_footprint.shp\",\n",
    "        \"./data/interim/image_footprints/test_Kalobeyei_2B_Flight_03_footprint.shp\",\n",
    "    ]\n",
    "    BACKGROUND_ANNOTATIONS_PATHS = [\n",
    "        CAR_POINTS_FILE,\n",
    "        WELL_POINTS_FILE,\n",
    "        HEDGE_POINTS_FILE,\n",
    "    ]\n",
    "    DATA_CRS = \"EPSG:32636\"\n",
    "\n",
    "    # Load all polygon annotations\n",
    "    polygons = gpd.read_file(POLYGON_ANNOTATIONS_PATH).to_crs(DATA_CRS)\n",
    "\n",
    "    # Filter for solar and building polygons\n",
    "    solar_conditions = {\n",
    "        \"power\": lambda x: x == \"generator\",\n",
    "        \"generator:source\": lambda x: x == \"solar\",\n",
    "    }\n",
    "    building_conditions = {\"building\": lambda x: ~x.isna()}\n",
    "\n",
    "    solar = _filter_dataframe(polygons, solar_conditions)\n",
    "    bldg = _filter_dataframe(polygons, building_conditions)\n",
    "    print(f\"Number of SOLAR polygons: {len(solar)}\")\n",
    "\n",
    "    # Get building polygons that do NOT contain solar polygons\n",
    "    bldg_contains_solar = gpd.sjoin(\n",
    "        bldg, solar, how=\"inner\", predicate=\"contains\"\n",
    "    )\n",
    "    bldg_no_solar = bldg[~bldg.index.isin(bldg_contains_solar.index)].copy()\n",
    "\n",
    "    # Subsample diverse set of building polygons\n",
    "    bldg_no_solar = _sample_diverse_values(\n",
    "        bldg_no_solar, \"building\", num_to_sample=150, seed=seed\n",
    "    )\n",
    "    print(f\"Number of BUILDING polygons: {len(bldg_no_solar)}\")\n",
    "\n",
    "    # Get centroids of solar and building polygons\n",
    "    solar[\"geometry\"] = solar.centroid\n",
    "    bldg_no_solar[\"geometry\"] = bldg_no_solar.centroid\n",
    "\n",
    "    # Sample points from footprints\n",
    "    footprint_points = _sample_points_from_footprints(\n",
    "        FOOTPRINT_PATHS, DATA_CRS, num_to_sample=400, seed=seed\n",
    "    )\n",
    "    # Load manually-selected background points\n",
    "    manual_points = concat_geo_files(BACKGROUND_ANNOTATIONS_PATHS, DATA_CRS)\n",
    "    # Combine footprint points and manual points\n",
    "    background = pd.concat([footprint_points, manual_points]).reset_index(\n",
    "        drop=True\n",
    "    )\n",
    "    background[\"background_idx\"] = background.index\n",
    "    # Exclude points that are too close to solar or building centroids\n",
    "    background = exclude_points_within_buffer(\n",
    "        background, pd.concat([solar, bldg_no_solar]), 20\n",
    "    )\n",
    "    print(f\"Number of BACKGROUND points: {len(background)}\")\n",
    "\n",
    "    return solar, bldg_no_solar, background\n",
    "\n",
    "def _find_raster_file_for_bbox(bbox, raster_files, footprint_files_dir=f'{INTERIM_DATA_DIR}/image_footprints'):\n",
    "    \"\"\"\n",
    "    Find the raster file that contains the bounding box\n",
    "    \"\"\"\n",
    "    bbox_left, bbox_bottom, bbox_right, bbox_top = bbox\n",
    "    bbox_polygon = shapely.geometry.box(bbox_left, bbox_bottom, bbox_right, bbox_top)\n",
    "    for raster_file in raster_files:\n",
    "        # Find the footprint file for the raster file\n",
    "        footprint_file = glob.glob(f'{footprint_files_dir}/*{os.path.basename(raster_file).replace(\".tif\", \"_footprint.shp\")}')\n",
    "        if len(footprint_file) == 0:\n",
    "            raise ValueError(f'No footprint file found for {raster_file}')\n",
    "        footprint_file = footprint_file[0]\n",
    "        \n",
    "        # Load the footprint file\n",
    "        footprint = gpd.read_file(footprint_file)\n",
    "        footprint = unary_union(footprint.geometry)\n",
    "        \n",
    "        # Check if the footprint completely contains the bounding box\n",
    "        if footprint.contains(bbox_polygon):\n",
    "            return raster_file\n",
    "    # If no raster file contains the bounding box, return None\n",
    "    return None\n",
    "\n",
    "# Import the sample_chips function from the sample_chips.py script\n",
    "sample_chips_path = Path('../scripts/semantic_segmentation/sample_chips.py')\n",
    "sample_chips = import_from_path('sample_chips', sample_chips_path)\n",
    "\n",
    "# Override the functions in the imported module\n",
    "sample_chips._get_candidate_points = _get_candidate_points\n",
    "sample_chips._find_raster_file_for_bbox = _find_raster_file_for_bbox\n",
    "\n",
    "# Import necessary functions\n",
    "_filter_dataframe = sample_chips._filter_dataframe\n",
    "_sample_diverse_values = sample_chips._sample_diverse_values\n",
    "_sample_points_from_footprints = sample_chips._sample_points_from_footprints\n",
    "\n",
    "args = argparse.Namespace(\n",
    "    seed=42,\n",
    "    chip_size=256,\n",
    "    input_imagery_dir = f'{RAW_DATA_DIR}/images',\n",
    "    input_mask_dir = f'{RAW_DATA_DIR}/semantic_segmentation/masks',\n",
    "    chip_output_dir = f'{PROCESSED_DATA_DIR}/chipped_datasets/semantic_segmentation',\n",
    "    chip_locations_save_path = f'{INTERIM_DATA_DIR}/semantic_segmentation/chip_locations.gpkg',\n",
    ")\n",
    "\n",
    "# If this cell was already run and there are already chips in the output directory,\n",
    "# remove them before re-sampling chips\n",
    "for chip_file in Path(args.chip_output_dir).rglob('*.tif'):\n",
    "    os.remove(chip_file)\n",
    "\n",
    "sample_chips.sample_chips(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_chips_dir = f'{PROCESSED_DATA_DIR}/chipped_datasets/semantic_segmentation/images'\n",
    "train_chips = glob.glob(f'{image_chips_dir}/trainval_Kalobeyei_2B_Flight_03_chip_*.tif')\n",
    "print(f\"Number of training chips: {len(train_chips)}\")\n",
    "test_chips = glob.glob(f'{image_chips_dir}/test_Kalobeyei_2B_Flight_03_chip_*.tif')\n",
    "print(f\"Number of test chips: {len(test_chips)}\")\n",
    "\n",
    "np.random.seed(42)\n",
    "random_train_chips = np.random.choice(train_chips, 10, replace=False)\n",
    "\n",
    "# Plot the 10 random image chips\n",
    "fig, axes = plt.subplots(2, 5, figsize=(20, 8))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for ax, chip in zip(axes, random_train_chips):\n",
    "    with rasterio.open(chip) as src:\n",
    "        image = src.read([1, 2, 3]).transpose(1, 2, 0)\n",
    "    mask_chip = chip.replace('images', 'masks')\n",
    "    with rasterio.open(mask_chip) as src:\n",
    "        mask = src.read(1)\n",
    "    ax.imshow(image)\n",
    "    ax.imshow(mask, cmap=segmentation_cmap, alpha=0.3, vmin=0, vmax=4)\n",
    "    ax.set_title(os.path.basename(chip))\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "\n",
    "In this section, we will train a U-Net model with a `resnext50_32x4d` backbone on the prepared dataset. The U-Net architecture is popular for semantic segmentation due to its ability to capture both spatial and contextual information, while the `resnext50_32x4d` backbone is a popular feature extractor. While we are using this specific setup, other architectures and backbones can also be used.\n",
    "\n",
    "For this demo, we will use cross-entropy loss, which is commonly used for multi-class classification problems. Cross-entropy loss measures the performance of a classification model whose output is a probability value between 0 and 1. However, other loss functions like Jaccard loss (also known as Intersection over Union loss) can be used to optimize the model further, especially in cases where the dataset is imbalanced or where the overlap between predicted and actual segments is critical. For this demo, we will use pixelwise cross-entropy loss, which measures the performance of a model by comparing the predicted class probability for each pixel to the true class label. Other loss functions like Jaccard loss can also be used for further optimization.\n",
    "\n",
    "Below is the configuration for the `CustomLogSemanticSegmentation` task used in our training script:\n",
    "\n",
    "```python\n",
    "task = CustomLogSemanticSegmentation(\n",
    "    model=\"unet\",\n",
    "    backbone=\"resnext50_32x4d\",\n",
    "    weights=\"imagenet\",  # use pretrained imagenet weights\n",
    "    in_channels=3,\n",
    "    num_classes=len(class_names) + 1,  # +1 for 0 \"not labeled\" class\n",
    "    loss=\"ce\",  # cross-entropy loss\n",
    "    class_weights=args.class_weights,\n",
    "    ignore_index=0,  # class 0 represents \"not labeled\" in the label masks\n",
    "    learning_rate=args.learning_rate,\n",
    "    learning_rate_schedule_patience=10,\n",
    "    train_metrics_file=base_path / \"train_metrics.csv\",\n",
    "    val_metrics_file=base_path / \"val_metrics.csv\",\n",
    "    test_metrics_file=base_path / \"test_metrics.csv\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the output directory for the model\n",
    "output_dir = './outputs'\n",
    "\n",
    "# Define the directories for the chipped images and masks\n",
    "chip_dir = f'{PROCESSED_DATA_DIR}/chipped_datasets/semantic_segmentation/images'\n",
    "mask_dir = f'{PROCESSED_DATA_DIR}/chipped_datasets/semantic_segmentation/masks'\n",
    "# Define the path to the tile split file\n",
    "tile_split_file = './config/dataset_splits.yml'\n",
    "\n",
    "# Construct the command to train the model\n",
    "command = (\n",
    "    f\"python ../scripts/semantic_segmentation/train.py \"\n",
    "    \"--exp-version demo \"\n",
    "    f\"--chip-dir {chip_dir} \"\n",
    "    f\"--mask-dir {mask_dir} \"\n",
    "    f\"--tile-split-file {tile_split_file} \"\n",
    "    f\"--output-dir {output_dir} \"\n",
    "    \"--max-epochs 2 \"\n",
    "    \"--gpu-id -1 \"\n",
    "    \"--num-workers 6\"\n",
    ")\n",
    "\n",
    "# Display and run the command\n",
    "print(command)\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation and Inference\n",
    "\n",
    "The `train.py` saves predictions for training, validation, and test set chips using the best model from a training run. Let's visualize a sample of those saved predictions from the most recent model training run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chip predictions\n",
    "output_dirs = sorted(Path(output_dir).glob('semantic_segmentation/demo-*-*-*'))\n",
    "latest_output_dir = \"./\" + str(output_dirs[-1]) # Get most recent output directory\n",
    "print(f\"Latest output directory: {latest_output_dir}\")\n",
    "chip_preds_f = list(Path(latest_output_dir).glob('epoch=*-step=*_chip_inference.pkl'))[0]\n",
    "with chip_preds_f.open('rb') as f:\n",
    "    chip_preds = pickle.load(f)\n",
    "\n",
    "# Plot 5 random chips with predictions\n",
    "np.random.seed(42)\n",
    "random_chips = np.random.choice(list(chip_preds['test_chips'].keys()), 5, replace=False)\n",
    "\n",
    "fig, axes = plt.subplots(5, 3, figsize=(8, 10))\n",
    "for i, chip_path in enumerate(random_chips):\n",
    "    chip_rel_path = chip_path.relative_to('./')\n",
    "    with rasterio.open(chip_rel_path) as ds:\n",
    "        chip_img = ds.read()[0:3].transpose(1, 2, 0)\n",
    "    mask_path = Path(*[part if part != 'images' else 'masks' for part in chip_rel_path.parts])\n",
    "    with rasterio.open(mask_path) as ds:\n",
    "        mask_img = ds.read().squeeze()\n",
    "    chip_pred = chip_preds['test_chips'][chip_path]\n",
    "    axes[i, 0].imshow(chip_img)\n",
    "    axes[i, 0].set_title('Image')\n",
    "    axes[i, 1].imshow(mask_img, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "    axes[i, 1].set_title('Mask')\n",
    "    axes[i, 2].imshow(chip_pred, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "    axes[i, 2].set_title('Prediction')\n",
    "    for ax in axes[i]:\n",
    "        ax.axis('off')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a real-world scenario, we would train the model for many more epochs, and possibly on a larger dataset. We've included outputs from training the model for 50 total epochs--let's compare the test chip predictions we got from that run with the ones we have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load chip predictions from model trained for 100 epochs\n",
    "max_epochs_100_output_dir = Path('./outputs/semantic_segmentation/demo-100-epochs')\n",
    "max_epochs_100_chip_preds_f = list(max_epochs_100_output_dir.glob('epoch=*-step=*_chip_inference.pkl'))[0]\n",
    "with max_epochs_100_chip_preds_f.open('rb') as f:\n",
    "    max_epochs_100_chip_preds = pickle.load(f)\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axes = plt.subplots(5, 4, figsize=(10, 10))\n",
    "\n",
    "# Iterate over random chips and plot the images, masks, and predictions\n",
    "for i, chip_path in enumerate(random_chips):\n",
    "    # Get the relative path of the chip\n",
    "    chip_rel_path = chip_path.relative_to('./')\n",
    "    \n",
    "    # Load the chip image\n",
    "    with rasterio.open(chip_rel_path) as ds:\n",
    "        chip_img = ds.read()[0:3].transpose(1, 2, 0)\n",
    "    \n",
    "    # Construct the mask path and load the mask image\n",
    "    mask_path = Path(*[part if part != 'images' else 'masks' for part in chip_rel_path.parts])\n",
    "    with rasterio.open(mask_path) as ds:\n",
    "        mask_img = ds.read().squeeze()\n",
    "    \n",
    "    # Get the predictions for the chip\n",
    "    chip_pred = chip_preds['test_chips'][chip_path]\n",
    "    max_epochs_100_chip_pred = max_epochs_100_chip_preds['test_chips'][chip_path]\n",
    "    \n",
    "    # Plot the chip image\n",
    "    axes[i, 0].imshow(chip_img)\n",
    "    axes[i, 0].set_title('Image')\n",
    "    \n",
    "    # Plot the mask image\n",
    "    axes[i, 1].imshow(mask_img, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "    axes[i, 1].set_title('Mask')\n",
    "    \n",
    "    # Plot the prediction\n",
    "    axes[i, 2].imshow(chip_pred, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "    axes[i, 2].set_title('Prediction')\n",
    "    \n",
    "    # Plot the prediction from the model trained for 100 epochs\n",
    "    axes[i, 3].imshow(max_epochs_100_chip_pred, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "    axes[i, 3].set_title('Prediction (100 epochs)')\n",
    "    \n",
    "    # Turn off axis for all subplots in the current row\n",
    "    for ax in axes[i]:\n",
    "        ax.axis('off')\n",
    "\n",
    "# Adjust layout to prevent overlap\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_test_metrics(file_path):\n",
    "    \"\"\"\n",
    "    Reads the test metrics from a CSV file and returns it as a dictionary.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str or Path): Path to the test metrics CSV file.\n",
    "    \n",
    "    Returns:\n",
    "        dict: Dictionary containing the test metrics.\n",
    "    \"\"\"\n",
    "    file_path = Path(file_path)\n",
    "    \n",
    "    # Read the single line from the file\n",
    "    with file_path.open('r') as f:\n",
    "        line = f.readline().strip()\n",
    "    \n",
    "    # Convert the line to a dictionary\n",
    "    metrics_dict = eval(line)\n",
    "    \n",
    "    return metrics_dict\n",
    "\n",
    "# Path to the test metrics file from 100-epoch model\n",
    "max_epochs_100_test_metrics_f = Path('./outputs/semantic_segmentation/demo-100-epochs/test_metrics.csv')\n",
    "max_epochs_100_test_metrics = read_test_metrics(max_epochs_100_test_metrics_f)\n",
    "\n",
    "# Path to the test metrics file from latest model\n",
    "output_dirs = sorted(Path(output_dir).glob('semantic_segmentation/demo-*'))\n",
    "latest_output_dir = output_dirs[-1]  # Get most recent output directory\n",
    "latest_test_metrics_f = Path(latest_output_dir) / 'test_metrics.csv'\n",
    "latest_test_metrics = read_test_metrics(latest_test_metrics_f)\n",
    "\n",
    "# Create a DataFrame from the test metrics\n",
    "model_run = ['demo-100-epochs', 'latest']\n",
    "multiclass_accuracy = [\n",
    "    max_epochs_100_test_metrics['test_MulticlassAccuracy'],\n",
    "    latest_test_metrics['test_MulticlassAccuracy']\n",
    "]\n",
    "multiclass_jaccard = [\n",
    "    max_epochs_100_test_metrics['test_MulticlassJaccardIndex'],\n",
    "    latest_test_metrics['test_MulticlassJaccardIndex']\n",
    "]\n",
    "metrics_df = pd.DataFrame({\n",
    "    'Model Run': model_run,\n",
    "    'Multiclass Accuracy': multiclass_accuracy,\n",
    "    'Multiclass Jaccard': multiclass_jaccard\n",
    "})\n",
    "\n",
    "# Display the DataFrame\n",
    "display(metrics_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use the best checkpoint from the latest model training run to perform inference on the train/val and test scenes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = list((Path(latest_output_dir) / 'checkpoints').glob('epoch=*-step=*.ckpt'))[0]\n",
    "checkpoint_f = f'./{checkpoint}'\n",
    "image_dir = './data/raw/images'\n",
    "inference_output_dir = f'{latest_output_dir}/inference'\n",
    "device = 'cpu'\n",
    "\n",
    "# Construct the command to run inference\n",
    "command = (\n",
    "    f\"python ../scripts/semantic_segmentation/inference.py \"\n",
    "    f\"--checkpoint {checkpoint_f} \"\n",
    "    f\"--image-dir {image_dir} \"\n",
    "    f\"--image-glob-pattern '*test_Kalobeyei_2B_Flight_03.tif' \"\n",
    "    f\"--output-dir {inference_output_dir} \"\n",
    "    f\"--device {device}\"\n",
    ")\n",
    "\n",
    "# Display and run the command\n",
    "print(command)\n",
    "subprocess.run(command, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scene predictions\n",
    "# Uncomment the following line to use predictions from the latest model training run:\n",
    "# scene_preds_f = list(Path(inference_output_dir).glob('*_test_Kalobeyei_2B_Flight_03.tif'))[0]\n",
    "\n",
    "# Uncomment the following line to use predictions from the 100 epoch model training run:\n",
    "scene_preds_f = list((max_epochs_100_output_dir / 'inference').glob('*_test_Kalobeyei_2B_Flight_03.tif'))[0]\n",
    "\n",
    "# Ensure the correct variable is used for loading the scene predictions\n",
    "with rasterio.open(scene_preds_f) as src:\n",
    "    scene_preds = src.read().squeeze()\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(5, 25))\n",
    "# Plot the entire scene predictions\n",
    "ax[0].imshow(scene_preds, cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "ax[0].set_title('Kalobeyei_2B Test Predictions', fontsize=10)\n",
    "# Plot a zoomed-in section of the predictions\n",
    "ax[1].imshow(scene_preds[7000:8000, 4000:5000], cmap=segmentation_cmap, vmin=0, vmax=4)\n",
    "ax[1].set_title('Zoomed-in Predictions', fontsize=10)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "turkana-camp-roof-mapping",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
